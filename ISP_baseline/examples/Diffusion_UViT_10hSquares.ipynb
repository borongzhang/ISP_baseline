{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c7c5e8-a6d1-4489-9486-e806cf2c8a58",
   "metadata": {},
   "source": [
    "# Diffusion Model (UViT) with overlapping squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37342931-ae7c-4596-a63b-c95753c0fc88",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6d6ef-f13d-4e87-be79-ec10d5754e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Mapping\n",
    "\n",
    "import inspect\n",
    "\n",
    "import functools\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import diffusion as dfn_lib\n",
    "from swirl_dynamics.lib import solvers as solver_lib\n",
    "from swirl_dynamics.projects import probabilistic_diffusion as dfn\n",
    "\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6824a0-725f-4174-b2b3-bc87afbd12da",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6732635-f566-4d12-98ea-a526c09738ac",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c0a30-2e86-45e5-a66e-f84ecec7c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Number of training datapoints.\n",
    "NTRAIN = 21000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4718dd-eaeb-45fd-b0d5-be2427a264e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata'\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{training_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :].T) for i in range(NTRAIN)]).astype('float32')\n",
    "    \n",
    "mean_eta = np.mean(eta_re, axis = 0)\n",
    "eta_re -= mean_eta\n",
    "std_eta = np.std(eta_re)\n",
    "eta_re /= std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{training_data_path}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "\n",
    "mean0, std0 = np.mean(scatter[:,:,:,0]), np.std(scatter[:,:,:,0])\n",
    "mean1, std1 = np.mean(scatter[:,:,:,1]), np.std(scatter[:,:,:,1])\n",
    "mean2, std2 = np.mean(scatter[:,:,:,2]), np.std(scatter[:,:,:,2])\n",
    "\n",
    "scatter[:,:,:,0] -= mean0\n",
    "scatter[:,:,:,0] /= std0\n",
    "scatter[:,:,:,1] -= mean1\n",
    "scatter[:,:,:,1] /= std1\n",
    "scatter[:,:,:,2] -= mean2\n",
    "scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace92d3-77eb-4b81-959e-3ac80f5cb977",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_train = np.reshape(scatter[:,:,:],(NTRAIN,80,80,6))\n",
    "eta_re_train = eta_re.reshape(-1, 80, 80, 1)\n",
    "assert eta_re_train.shape == (NTRAIN, 80, 80, 1)\n",
    "assert scatter_train.shape == (NTRAIN, 80, 80, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef93394-ed5e-4255-bb48-7645988bb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "batch_size = 16\n",
    "dict_data = {\"x\": eta_re_train}\n",
    "dict_data[\"cond\"] = {\"channel:scatter\": scatter_train}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = eval_dataloader = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87548f03-8a65-4893-b37b-d014ab5ac58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = np.std(eta_re_train)\n",
    "data_size = len(eta_re_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0412bda-e25c-41b8-a673-7c515b0ef7cc",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebc3c5-272b-4477-9f7b-8b121c2ebda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_denoiser_model = dfn_lib.unets.PreconditionedDenoiser(\n",
    "    out_channels=1,\n",
    "    num_channels=(64, 128),\n",
    "    downsample_ratio=(2, 2),\n",
    "    num_blocks=4,\n",
    "    noise_embed_dim=128,\n",
    "    padding=\"SAME\",\n",
    "    use_attention=True,\n",
    "    use_position_encoding=True,\n",
    "    num_heads=8,\n",
    "    sigma_data=data_std,\n",
    "    cond_resize_method=\"cubic\",\n",
    "    cond_embed_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b3c2f-0a37-4814-a7e1-8c3b908647ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_scheme = dfn_lib.Diffusion.create_variance_preserving(\n",
    "    sigma=dfn_lib.tangent_noise_schedule(),\n",
    "    data_std=data_std,\n",
    ")\n",
    "\n",
    "cond_model = dfn.DenoisingModel(\n",
    "    input_shape=(80, 80, 1),\n",
    "    # `cond_shape` must agree with the expected structure and shape\n",
    "    # (without the batch dimension) of the `cond` input.\n",
    "    cond_shape={\"channel:scatter\": (80, 80, 6)},\n",
    "    denoiser=cond_denoiser_model,\n",
    "    noise_sampling=dfn_lib.time_uniform_sampling(\n",
    "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
    "    ),\n",
    "    noise_weighting=dfn_lib.edm_weighting(data_std=data_std),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea110d-5475-49e2-97d2-e659e83407f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -R -f $cond_workdir  # optional: clear the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d6915-ae0e-4e45-bfb9-c392bd9633f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = data_size * epochs // batch_size  #@param\n",
    "cond_workdir = os.path.abspath('') + \"/tmp/diffusion_uvit_squares\"\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-4 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-7 #@param\n",
    "ema_decay = 0.999  #@param\n",
    "ckpt_interval = 2000 #@param\n",
    "max_ckpt_to_keep = 3 #@param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5956f0e-960b-4fdf-a3c4-176919b2850e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95390caa-9cfa-44e8-8796-1e9409898f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_trainer = dfn.DenoisingTrainer(\n",
    "    model=cond_model,\n",
    "    rng=jax.random.PRNGKey(888),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    "    ema_decay=ema_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61300e-6a0a-42ae-9350-0d3e160b92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=cond_trainer,\n",
    "    workdir=cond_workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        cond_workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps=100,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    eval_every_steps = 1000,\n",
    "    num_batches_per_eval = 2,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=cond_workdir,\n",
    "            options=ocp.CheckpointManagerOptions( \n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcd5c5-6b90-4a4e-bf0a-e1ebab9a870d",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0c1bd-fc41-470d-8ee0-8faa6c147b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "NTEST = 50\n",
    "test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTEST, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :].T) for i in range(NTEST)]).astype('float32')\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTEST, :]\n",
    "    tmp2 = f[keys[4]][:NTEST, :]\n",
    "    tmp3 = f[keys[5]][:NTEST, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTEST, :]\n",
    "    tmp2 = f[keys[1]][:NTEST, :]\n",
    "    tmp3 = f[keys[2]][:NTEST, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter_t = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "scatter_t[:,:,:,0] -= mean0\n",
    "scatter_t[:,:,:,0] /= std0\n",
    "scatter_t[:,:,:,1] -= mean1\n",
    "scatter_t[:,:,:,1] /= std1\n",
    "scatter_t[:,:,:,2] -= mean2\n",
    "scatter_t[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac884eb7-0c36-4cdc-bccd-ad61499e3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_test = np.reshape(scatter_t[:,:,:],(NTEST,80,80,6))\n",
    "eta_re_test = eta_re.reshape(-1, 80, 80, 1)\n",
    "assert eta_re_test.shape == (NTEST, 80, 80, 1)\n",
    "assert scatter_test.shape == (NTEST, 80, 80, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6408b4-6c43-464a-a447-b6e00213fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to add noise to the scatter conditional\n",
    "#scatter_test_noised = scatter_test + c*np.max(np.absolute(scatter_test))*np.random.normal(size=scatter_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c43c1-77cb-4aca-aa4f-dcc2dca9513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
    "    f\"{cond_workdir}/checkpoints\", step=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8ae0e-a91b-4f67-87f7-e32f1b47f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the inference function\n",
    "cond_denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
    "    trained_state, use_ema=True, denoiser=cond_denoiser_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39f70d-72e2-4e62-8af9-d01b11a2b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_sampler = dfn_lib.SdeSampler(\n",
    "    input_shape=(80, 80, 1),\n",
    "    integrator=solver_lib.EulerMaruyama(),\n",
    "    tspan=dfn_lib.exponential_noise_decay(\n",
    "        diffusion_scheme, num_steps=256, end_sigma=1e-3,\n",
    "    ),\n",
    "    scheme=diffusion_scheme,\n",
    "    denoise_fn=cond_denoise_fn,\n",
    "    guidance_transforms=(),\n",
    "    apply_denoise_at_end=True,\n",
    "    return_full_paths=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854fea5-d21a-4bfb-b2af-bf6950b1c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_cond = 10\n",
    "\n",
    "generate = jax.jit(\n",
    "    functools.partial(cond_sampler.generate, num_samples_per_cond)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7289d-b5a0-4ea7-9786-0a855be511f5",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1375ea-856e-4d9e-9a75-ca1252cdd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 5\n",
    "dict_data_test = {}\n",
    "dict_data_test[\"cond\"] = {\"channel:scatter\": scatter_test}\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(dict_data_test)\n",
    "dataset_test = dataset_test.batch(batch_size_test)\n",
    "dataset_test = dataset_test.prefetch(tf.data.AUTOTUNE)\n",
    "dataset_test = dataset_test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e63d95-6d78-4ff2-8a0a-1f2e6e94b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_pred = np.zeros((NTEST, num_samples_per_cond, neta, neta, 1))\n",
    "\n",
    "b = 0\n",
    "for batch in dataset_test:\n",
    "    #for i in range(10):\n",
    "    print(f\"\\rProcessing batch {b + 1} / {NTEST//batch_size_test}\", end='', flush=True)\n",
    "    cond_samples = jax.device_get(jax.vmap(generate, in_axes=(0, 0, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(888), batch_size_test),\n",
    "        batch[\"cond\"],\n",
    "        None,  # Guidance inputs = None since no guidance transforms involved\n",
    "    ))\n",
    "    #print(b)\n",
    "    eta_pred[b*batch_size_test:(b+1)*batch_size_test,:,:,:,:] = cond_samples*std_eta+mean_eta[:, :, jnp.newaxis]\n",
    "    b += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba385494-d03c-45ab-b2c0-72544014ad4f",
   "metadata": {},
   "source": [
    "### Plotting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293c769-2f67-4e46-8aa9-f7a0e99cb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_cond_plot = 4\n",
    "fig = plt.figure(layout='constrained', figsize=(10, 16))\n",
    "subfigs = fig.subfigures(2, 1, wspace=0.07)\n",
    "for i in range(2):\n",
    "    #subfig = fig.subfigures(layout='constrained', figsize=(10, 4))\n",
    "    subfigsnest = subfigs[i].subfigures(2, 1)#, wspace=0.1)\n",
    "    #print(subfigs.shape)\n",
    "    subfigsnest1 = subfigsnest[0].subfigures(1, 2)#, height_ratios=[1, 1.4])\n",
    "    axsLeft = subfigsnest1[0].subplots(2,3)\n",
    "    subfigsnest1[0].set_facecolor('0.75')\n",
    "    l = 0\n",
    "    for j in range(2):\n",
    "        for k in range(3):\n",
    "            im = axsLeft[j,k].imshow(scatter_test[i, :, :, l])#,vmin=-2.5, vmax=2.5)\n",
    "            #print(test_batch_cond_denorm[i, :, :, l].shape)\n",
    "            l += 1\n",
    "            axsLeft[j,k].axis(\"off\")\n",
    "    subfigsnest1[0].suptitle(f\"Scatter condition: #{i + 1}\", fontsize='x-large')\n",
    "    subfigsnest1[0].colorbar(im,shrink=0.6, ax=axsLeft, location='right')\n",
    "    \n",
    "    axsRight = subfigsnest1[1].subplots(1, 1)\n",
    "    im_eta = axsRight.imshow(eta_re_test[i, :, :, 0])\n",
    "    \n",
    "    #subfigs[1].set_facecolor('0.85')\n",
    "    #subfigs[1].colorbar(pc, shrink=0.6, ax=axsRight)\n",
    "    subfigsnest1[1].suptitle('Ground truth', fontsize='x-large')\n",
    "    subfigsnest1[1].colorbar(im_eta,shrink=0.6, ax=axsRight)\n",
    "    #fig.suptitle('Scatter and ground truth', fontsize='xx-large')\n",
    "    \n",
    "    # Plot generated samples.\n",
    "    ax = subfigsnest[1].subplots(\n",
    "              2, int(num_samples_per_cond_plot/2))#, figsize=(num_samples_per_cond_plot*2, 4))\n",
    "    l = 0\n",
    "    for k in range(2):\n",
    "        for j in range(int(num_samples_per_cond_plot/2)):\n",
    "            im = ax[k,j].imshow(eta_pred[i,l,:,:,0])\n",
    "                 #cond_samples_denorm_[i, l, :, :, 0])#, vmin=0, vmax=2.5)\n",
    "            ax[k,j].set_title(f\"conditional sample: #{l+1}\")\n",
    "            ax[k,j].axis(\"off\")\n",
    "            l += 1\n",
    "    subfigsnest[1].colorbar(im, ax=ax[:,:], shrink=0.6, location=\"right\")\n",
    "        \n",
    "    #plt.tight_layout()\n",
    "#fig.savefig('cond_smooth_sigma_0.04_freq_2.5.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f56d21-a5ef-4045-bf3d-0acd87f4a600",
   "metadata": {},
   "source": [
    "### Quick metric check (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f38d14-f036-4464-b781-7ce5670942c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(NTEST):\n",
    "    for j in range(num_samples_per_cond):\n",
    "        errors.append(np.linalg.norm(eta_re_test[i,:,:,0]-eta_pred[i,j,:,:,0])/np.linalg.norm(eta_re_test[i,:,:,0]))\n",
    "        \n",
    "print('Mean of validation relative l2 error:', np.mean(errors))\n",
    "print('Median of validation relative l2 error:', np.median(errors))\n",
    "print('Min of validation relative l2 error:', np.min(errors))\n",
    "print('Max of validation relative l2 error:', np.max(errors))\n",
    "print('Standard deviation of validation relative l2 errors:', np.std(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77184f7-f4f4-4c95-9560-bdab47179bf3",
   "metadata": {},
   "source": [
    "### Writing files for restults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452402d-038f-42cf-9ffc-b48c109be37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"Results/diffusion_CRPS_squares_freq_2.5_5_10.hdf5\", \"w\") as f:\n",
    "    eta = f.create_dataset('eta', data=eta_re_test)\n",
    "    eta_predicted = f.create_dataset('eta_pred', data=eta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d85e4-8a19-42d2-9d9b-642a81bfb169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
